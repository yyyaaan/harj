---
title: "Stochastic Simulation | Problems 4"
author: " Yan PAN"
output: 
  html_document: 
    df_print: paged 
    theme: 'readable'
---

<style>
h1,h2,h3{
  font-family: Georgia,"Times New Roman",Times,serif !important;
  padding-top: 20px;
}
blockquote{
  font-size: 16px; !important;
}
</style>

```{r setup, include=FALSE} 
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE, 
                      warning = FALSE, 
                      comment = "") 
myprint <- function(printD, dig = 4)
  data.frame(round(printD, digits = dig)) 
```

##Exercise 1
<blockquote>
Prove law of total variance,

$$\text{Var}(X) = \mathbb E [ \text{Var}(X|Y)] + \text{Var}(\mathbb E[X|Y])$$
</blockquote>
$$\begin{aligned} \text{Right side}
&= \mathbb E [ \text{Var}(X|Y)] + \text{Var}(\mathbb E[X|Y])
\\ &= \mathbb E \bigg[ \mathbb E[X^2|Y] - (\mathbb E[X|Y])^2  \bigg] + \mathbb E \bigg[ (\mathbb E[X|Y])^2 \bigg] - \bigg( \mathbb E[\mathbb E [X|Y]] \bigg)^2
\\ &= \mathbb E \bigg[ \mathbb E[X^2|Y]  \bigg] - \mathbb E \bigg[ (\mathbb E[X|Y])^2  \bigg] + \mathbb E \bigg[ (\mathbb E[X|Y])^2 \bigg] - \bigg( \mathbb E[\mathbb E [X|Y]] \bigg)^2
\\ &= \mathbb E \bigg[ \mathbb E[X^2|Y]  \bigg]  - \bigg( \mathbb E[\mathbb E [X|Y]] \bigg)^2
\\ &= \mathbb E [X^2] - (\mathbb E [X])^2 &\text{by tower property}
\\ &= \text{Var}(X)
\end{aligned}$$

##Exercise 2
<blockquote>
Metropolis-Hastings transition probability $K$ is reversible with respect to $p$. Suppose $\mathbb X$ is countable.
</blockquote>

$$\begin{aligned} \text{Case 1: suppose }x=y
\\ p(x)K(x,y) &=p(x) \bigg( q(x,y) \alpha(x,y) + 1 - \sum\limits_{y\in\mathbb X}q(x,y)\alpha(x,y)\bigg)
\\  &=p(y) \bigg( q(y,x) \alpha(y,x) + 1 - \sum\limits_{y\in\mathbb X}q(y,x)\alpha(y,x)\bigg) &\text{since }x=y
\\ &= p(y) K(y,x)
\\
\\ \text{Case 2: suppose }x\neq y, &\ q(x,y)\gt 0, q(y,x)\gt 0
\\ p(x)K(x,y) &= p(x) q(x,y) \alpha(x,y) 
\\ &= \min \bigg\{ p(x)q(x,y), p(x)q(x,y) \frac{p(y)q(y,x)}{p(x)q(x,y)} \bigg\}
\\ & = \min \bigg\{ p(x)q(x,y), p(y)q(y,x) \bigg\}
\\ &= p(y)q(y,x) \min \bigg\{ \frac{p(x)q(x,y)}{p(y)q(y,x)}, 1 \bigg\}
\\ &= p(y)q(y,x)\alpha(y,x)
\\ &= p(y)K(y,x)
\\
\\ \text{Case 3: suppose }x\neq y &\text{ and } q(x,y)= 0 \text{ or } q(y,x)= 0
\\ \Rightarrow &\begin{cases} q(x,y) = 0\ \Rightarrow\ \alpha(x,y)=0 \\ q(y,x) = 0\ \Rightarrow\ \alpha(x,y)=\min\{0,1\} = 0 \end{cases}
\\ \Rightarrow & p(x)K(x,y) = 0 = p(y)K(y,x)
\end{aligned}$$

Therefore, $p(x)K(x,y) =p(y)K(y,x),\ \forall x\in\mathbb X$, which concludes that $K$ is $p$-reversible.

##Exercise 3
<blockquote>
Metropolis-Hastings algorithm targets to Binom$(m,\beta)$
</blockquote>

Note that the proposal distribution $q(k,j)=(m+1)^{-1}\sim\mathcal U(\{0,1,2,\dots,m\})$ is a discrete uniform distribution, and it is actually regardless of the previous value nor the current one.

The `myMH` function works with any symmetric proposal function.

```{r demo43}
myMH <- function(n, x, logpFun, propFun, debug = FALSE){
  xs <- numeric(n)
  accept <- 0
  l_px <- logpFun(x)
  
  for (i in 1:n) {
    # y from proposal
    y <- propFun(x)
    l_py <- logpFun(y)
    # acceptance?
    alpha <- exp(l_py - l_px)
    if(runif(1) < alpha){
      accept <- accept + 1 #set the acceptance ratio
      x <- y
      l_px <- l_py
    }
    xs[i] <- x
  }
  
  if(debug){plot(xs, type = "l");  print(xs)}

  list(est = sum(xs)/n,
       var = var(xs),
       accept = accept/n)
}

myMH(1e5, 99, 
     logpFun = function(x) dbinom(x, 100, 0.9, log=TRUE),
     propFun = function(x) floor(runif(1, 0, 101)))
```

##Exercise 4
<blockquote>
Same as exercise 3, but with different proposal distribution.
</blockquote>

The proposal is generated by $Y_k = X_{k-1} +[Z_k]$, indicating symmetry, i.e. $q(x,y) = q(y,x)$. The `myMH` funciton created in Exercise 3 still valid here.

```{r demo44}
propFun <- function(x){
  m <- 100
  beta <- 0.9
  z <- rnorm(1, mean = 0, sd = sqrt(5*m*beta*(1-beta)))
  x + round(z)
}

myMH(1e5, 99, 
     logpFun = function(x) dbinom(x, 100, 0.9, log=TRUE),
     propFun = propFun)
```

The mean, naturally, stays the same, while the variance is slightly larger. On the other hand, the acceptance ration seems greatly increased

##Exercise 5
<blockquote>
Study the model $V\sim U(0,10)$ and $Y_i|V \overset{iid}\sim N(0,V)$, and the target is the distirbution $p(v):\ V|Y_1=y_1,\dots,Y_{10}=y_{10}$, where $y_i$ are given values (see code).

Use MCMC to simulate with different proposal:

a. $V_k'=V_{k-1}+W_k,\ \ \ W_k\overset{iid}\sim N(0,0.1^2)$

</blockquote>

In this proposal,$q(V_k, V_{k-1}) = q(V_{k-1}, V_k)$ (easy task then; use the `myMH` function created above)

The $\propto$ does not matter in our simulation, since the quotient of the likelihoods. 

```{r demo45a}
logP <- function(v){
  if(v < 0 | v > 10) return(-Inf) # from U(0,10)
  
  givenY <- c(0.32, 0.27, 0.24, -0.09, -0.15, 0.43, -0.53, -0.38, -0.01, -0.04)
  py <- dnorm(givenY, mean = 0, sd = sqrt(v), log = TRUE)
  return(sum(py) + log(0.1))
}


myMH(1e5, 3, 
     logpFun = logP,
     propFun = function(v) v + rnorm(1, 0, 0.1))
```


> b. $\log V_k'=\log V_{k-1}+W_k,\ \ \ W_k\overset{iid}\sim N(0,0.1^2)$

The distribution of $p(\log(V_k),\log(V_{k-1})$ is still symmetric. The idea is to simulate for the log-distribution, which is possible because $V\geq 0$ must hold.

```{r demo45b}
logPlog <- function(logv){
  if( logv > log(10)) return(-Inf) # from U(0,10)
  
  givenY <- c(0.32, 0.27, 0.24, -0.09, -0.15, 0.43, -0.53, -0.38, -0.01, -0.04)
  py <- dnorm(givenY, mean = 0, sd = sqrt(exp(logv)), log = TRUE)
  return(sum(py) + log(0.1))
}

logSim <- myMH(1e5, log(0.001), 
               logpFun = logPlog,
               propFun = function(logv) logv + rnorm(1, 0, 0.1))
exp(logSim$est)
```

##Exercise 6
<blockquote>
$p_u$ unnormalized p.d.f., which is continuous, increasing in $(-\infty,\beta]$ and decreasing in $[\beta, \infty)$.

Consider $\tilde p_u(x,t):= \boldsymbol 1 (0 \leq t \leq pu(x))$ in $\mathbb R^2$.

a. Sketch the set $\{(x,t)\in\mathbb R^2: 0\leq t\leq p_u(x)\}$
</blockquote>

It is similar to the area between the $p_u$ and the $x$-axis (like the integral -> c.d.f)

> b. Describe a Gibbs sampling algorithm $(X_k, T_k)))_{k \geq 1}$ that targets to $\tilde p_u(x,t)$.

1. initial value/vector $(X_0, T_0)$, where $p_u(X_0)\neq 0,\ \ 0\lt T_0 \lt \max(p_u) = p_u(\beta)$ 

2. calculate $x_-$ and $x_+$ such that $p_u(x_-)=T_{k-1}=p_u(x_+)$

3. propose or randomly pick $(X_p, T_p)$

4. accept if $x_- \leq X_p \leq x_+$ and $T_p \gt T_{k-1}$, i.e. set $(X_k, T_k) = (X_p, T_p)$; otherwise $(X_k, T_k) = (X_{k-1}, T_{k-1})$

5. repeat 2-4

Note: in the initial step choose a really small $T_0$ since we don't the maximum. In case of proposal, the new $T_k$ should be greater but not "very different" from $T_{k-1}$. Including the boundary in proposal will surely increase the acceptance ratio.

> c. Where does $\frac 1 n \sum \limits_{k=1}^{n} f(X_k)$ converge to?

$\frac 1 n \sum \limits_{k=1}^{n} f(X_k)\overset{n\to\infty}\longrightarrow f(\beta)$, i.e. the maximum.